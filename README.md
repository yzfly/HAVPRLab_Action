# HAVPRLab DeepAction Learning 
Human Activity & Visual Perception Research Lab (HAVPRLab) DeepAction Learning Resources.

âœ¨æŒç»­æ›´æ–°ä¸­, æ¬¢è¿æäº¤ PR âœ¨

![License](https://img.shields.io/badge/license-MIT-yellow)

## ğŸ·ï¸ Learning Resources


#### åŸºç¡€
* [çŸ¥åçš„å´æ©è¾¾æ·±åº¦å­¦ä¹ æ•™ç¨‹](https://mooc.study.163.com/university/deeplearning_ai#/c)
* [Fast.ai å‡ºå“çš„æ·±åº¦å­¦ä¹ åŸºç¡€æ•™ç¨‹](https://www.fast.ai/)
* [æ·±åº¦å­¦ä¹ ä¸Šæ‰‹æŒ‡å—](https://github.com/nndl/nndl.github.io/blob/master/md/DeepGuide.md)
* **[å¦‚ä½•è¯»è®ºæ–‡-ææ²](https://www.bilibili.com/video/BV1H44y1t75x)** [BiliBili](https://www.bilibili.com/video/BV1H44y1t75x)
* ææ²å¤§ç¥å›¢é˜Ÿå‡ºå“çš„ç²¾è¯»è®ºæ–‡ç³»åˆ— [[BiliBili]](https://space.bilibili.com/1567748478/channel/collectiondetail?sid=32744) [[GitHub]](https://github.com/mli/paper-reading) 
    * [ResNetè®ºæ–‡é€æ®µç²¾è¯»](https://www.bilibili.com/video/BV1P3411y7nn)(åŸºç¡€)
    * [Transformerè®ºæ–‡é€æ®µç²¾è¯»](https://www.bilibili.com/video/BV1pu411o7BE)ï¼ˆåŸºç¡€ï¼‰
    * [ViTè®ºæ–‡é€æ®µç²¾è¯»](https://www.bilibili.com/video/BV15P4y137jb)ï¼ˆåŸºç¡€)
    * more ...

#### Action Recognition
* [è§†é¢‘ç†è§£è®ºæ–‡ä¸²è®²ï¼ˆä¸Šï¼‰](https://www.bilibili.com/video/BV1fL4y157yA)ğŸ”¥
* [è§†é¢‘ç†è§£è®ºæ–‡ä¸²è®²ï¼ˆä¸‹ï¼‰](https://www.bilibili.com/video/BV11Y411P7ep)ğŸ”¥
* [åŒæµç½‘ç»œè®ºæ–‡é€æ®µç²¾è¯»](https://www.bilibili.com/video/BV1mq4y1x7RU)ğŸ”¥
* [I3D è®ºæ–‡ç²¾è¯»](https://www.bilibili.com/video/BV1tY4y1p7hq)ğŸ”¥



## ğŸ·ï¸ Paper Lists
### Action Recognition
* [awesome-action-recognition](https://github.com/jinwchoi/awesome-action-recognition)(Action Recognition è®ºæ–‡åˆé›†)ğŸ”¥

* [TSN (ECCV 2016)](https://arxiv.org/abs/1608.00859) [[Code](https://github.com/yjxiong/temporal-segment-networks)] â­
* [I3D (CVPR 2017)](https://arxiv.org/abs/1705.07750) [[Code: kinetics-i3d](https://github.com/deepmind/kinetics-i3d)][[Code:pytorch-i3d](https://github.com/piergiaj/pytorch-i3d)] â­
* [3D-ResNets (CVPR 2018)](https://openaccess.thecvf.com/content_cvpr_2018/html/Hara_Can_Spatiotemporal_3D_CVPR_2018_paper.html) [[Code](https://github.com/kenshohara/3D-ResNets-PyTorch)] â­
* [TSM (ICCV 2019) ](http://arxiv.org/abs/1811.08383) [[Paper](https://openaccess.thecvf.com/content_ICCV_2019/papers/Lin_TSM_Temporal_Shift_Module_for_Efficient_Video_Understanding_ICCV_2019_paper.pdf)][[Code](https://github.com/mit-han-lab/temporal-shift-module)] â­
* [TEA (CVPR 2020)](https://arxiv.org/abs/2004.01398) [[Code](https://github.com/Phoenix1327/tea-action-recognition)]
* [SlowFast (ICCV 2019)](https://arxiv.org/abs/1812.03982) [[Paper](https://openaccess.thecvf.com/content_ICCV_2019/papers/Feichtenhofer_SlowFast_Networks_for_Video_Recognition_ICCV_2019_paper.pdf)] [[Code: official](https://github.com/facebookresearch/SlowFast)] [Code: mmaction2](https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/slowfast/README.md) â­
* [X3D (CVPR 2020)](https://arxiv.org/abs/2004.04730) [[Paper](https://openaccess.thecvf.com/content_CVPR_2020/html/Feichtenhofer_X3D_Expanding_Architectures_for_Efficient_Video_Recognition_CVPR_2020_paper.html)] [[Code: official](https://github.com/facebookresearch/SlowFast)] [[Code: mmaction2](https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/x3d/README.md)]
* [TDN (CVPR 2021)](https://arxiv.org/abs/2012.10071) [[Paper](https://arxiv.org/abs/2012.10071)] [[Code](https://github.com/MCG-NJU/TDN)] â­
* [TimeSformer (ICML 2021)](https://arxiv.org/pdf/2102.05095.pdf) [[Code](https://github.com/facebookresearch/TimeSformer)] 
* [Motionformer (NeurIPS 2021)](https://facebookresearch.github.io/Motionformer/) [[Paper](https://arxiv.org/abs/2106.05392)] [[Code](https://github.com/facebookresearch/Motionformer)]
* [Video Swin Transformer (CVPR 2022)](https://arxiv.org/abs/2106.13230) [[Paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Video_Swin_Transformer_CVPR_2022_paper.Paper)] [[Code](https://github.com/SwinTransformer/Video-Swin-Transformer)]â­

* [FineDiving: A Fine-grained Dataset for Procedure-aware Action Quality Assessment](https://finediving.ivg-research.xyz/) [[Paper](https://arxiv.org/pdf/2204.03646.pdf)] [[Code & Dataset](https://github.com/xujinglin/FineDiving)] (CVPR 2022 Oral | æ¸…åå¼€æºFineDivingï¼šç»†ç²’åº¦åŠ¨ä½œè´¨é‡è¯„ä¼°æ•°æ®é›†)
* [Expanding Language-Image Pretrained Models for General Video Recognition](https://github.com/microsoft/VideoX/tree/master/X-CLIP) [[Paper](https://arxiv.org/abs/2208.02816)] [[Code](https://github.com/microsoft/videox)] (ECCV 2022 Oral | å¾®è½¯å¼€æº X-Clip,åŠ¨ä½œè¯†åˆ«ï¼Œå°æ ·æœ¬å­¦ä¹ )
* [UniFormer: Unified Transformer for Efficient Spatiotemporal Representation Learning
](https://github.com/microsoft/VideoX/tree/master/X-CLIP) [[Paper](https://arxiv.org/abs/2201.04676)] [[Code](https://github.com/Sense-X/UniFormer)] (Uniformer ICLR2022 (è¯„åˆ† 8868, Top 3%), æ¯”è¾ƒæœ‰æ„æ€çš„ CNN ä¸ Transformer ç›¸äº’å¯å‘çš„å·¥ä½œï¼Œä½œè€…ä¹Ÿä½¿ç”¨ Uniformer æ‰“äº† CVPR dark action recognition çš„æ¯”èµ›)

### Others
* [ConvGRU (ICLR 2016)](https://arxiv.org/abs/1511.06432) [[Paper]((https://arxiv.org/abs/1511.06432))
]
* [ACmix (CVPR 2022)](https://arxiv.org/abs/2111.14556) [[Paper](https://arxiv.org/pdf/2111.14556v1.pdf)][[Code](https://github.com/LeapLabTHU/ACmix)] (DenseNet ä¸€ä½œé»„é«˜è€å¸ˆç»„ CNNä¸transformer èåˆçš„å·¥ä½œ)

## ğŸ·ï¸ Training Skills
* [PyTorch æŠ€å·§](https://github.com/lartpang/PyTorchTricks)ğŸ”¥
* [PyTorch ç‚¼ä¸¹è¿‡ç¨‹å¸¸ç”¨å°ä»£ç ](pytorch_snippets.md)
* [SWA](https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/) (ğŸ”¥æ— ç—›æ¶¨ç‚¹è®­ç»ƒæ–¹æ³•)
* [EMA](https://github.com/lucidrains/ema-pytorch) (ğŸ”¥æŒ‡æ•°æ»‘åŠ¨å¹³å‡æ— ç—›æ¶¨ç‚¹)
* [Fast.ai æ¨å´‡çš„ One Cycle è®­ç»ƒç­–ç•¥](https://fastai1.fast.ai/callbacks.one_cycle.html)
* [è°ƒå‚-å¦‚ä½•ç¡®å®šå­¦ä¹ ç‡ lr](https://www.yuque.com/explorer/blog/sv37zs)
* [Label Smoothing](https://github.com/pytorch/pytorch/issues/7455)
    * [torch.nn.CrossEntropyLoss å·²æ”¯æŒ](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)
    * ä¸€ä¸ª PyTorch å¤šåˆ†ç±»ç®€å•å®ç°
    ```
    class LabelSmoothingLoss(nn.Module):
        def __init__(self, classes, smoothing=0.0, dim=-1):
            super(LabelSmoothingLoss, self).__init__()
            self.confidence = 1.0 - smoothing
            self.smoothing = smoothing
            self.cls = classes
            self.dim = dim
            
            def forward(self, pred, target):
                pred = pred.log_softmax(dim=self.dim)
                with torch.no_grad():
                    # true_dist = pred.data.clone()
                    true_dist = torch.zeros_like(pred)
                    true_dist.fill_(self.smoothing / (self.cls - 1))
                    true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)
            return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))
    ```


##  ğŸ·ï¸ Github Repos
### Action-Related
* [mmaction2](https://github.com/open-mmlab/mmaction2)(çŸ¥åæ¡†æ¶ï¼ŒåŒ…å«åŠ¨ä½œè¯†åˆ«ç®—æ³•å¤š)ğŸ”¥
* [TSM](https://github.com/mit-han-lab/temporal-shift-module)(åŠ¨ä½œè¯†åˆ« 2DCNN ç»å…¸ç®—æ³•)
* [3D-ResNets-PyTorch](https://github.com/kenshohara/3D-ResNets-PyTorch)(åŠ¨ä½œè¯†åˆ« 3DCNN ç»å…¸ç®—æ³•)ğŸ”¥
* [Video-Swin-Transformer](https://github.com/SwinTransformer/Video-Swin-Transformer) (å½“çº¢è¾£å­é¸¡Transformer )ğŸ”¥
* [MARS](https://github.com/craston/MARS) (çŸ¥è¯†è’¸é¦ç®—æ³•)
* [IG65M](https://github.com/moabitcoin/ig65m-pytorch) (Models and weights pre-trained on 65MM Instagram videos.)

### Optical Flow
* [denseflow](https://github.com/open-mmlab/denseflow)(TVL1ç­‰å…‰æµæå–)
* [mmflow](https://github.com/open-mmlab/mmflow)( å•†æ±¤å¼€æºçš„å…‰æµæå–ä»£ç åº“ï¼ŒåŒ…å«å¤šç§çŸ¥åç®—æ³•)
* [PWCNet](https://github.com/NVlabs/PWC-Net)( PWC-Net å…‰æµå®˜æ–¹å®ç°)
* [RAFT](https://github.com/princeton-vl/RAFT)(ECCV2020 Best Paper æ·±åº¦å­¦ä¹ é«˜è´¨é‡å…‰æµæå–)

### Dataset-Related
* [Common Visual Data Foundation](https://github.com/cvdfoundation) (Kinetics400/600/700ã€AVA ç­‰å¤§å‹æ•°æ®é›†ä¾¿åˆ©ä¸‹è½½)
* [VoTT](https://github.com/microsoft/VoTT) (å¾®è½¯å‡ºå“çš„å¥½ç”¨çš„æ ‡æ³¨å·¥å…·) [[BiliBili](https://www.bilibili.com/video/BV1854y127gT)]
### å…¶ä»–ä»£ç åº“
* [External-Attention-pytorch](https://github.com/xmu-xiaoma666/External-Attention-pytorch)(å„ç§ Attention æœºåˆ¶çš„æ ¸å¿ƒå®ç°ï¼Œç®€å•æ˜“æ‡‚)ğŸ”¥
* [Timm](https://github.com/rwightman/pytorch-image-models) (å„ç§çŸ¥å Backbone å®ç°) ğŸ”¥


## ğŸ·ï¸  Useful Tools

* [decord](https://github.com/dmlc/decord) (é«˜æ€§èƒ½è§†é¢‘è¯»å–åº“)
* [profile](https://github.com/shibing624/python-tutorial/blob/master/06_tool/profiler%E5%B7%A5%E5%85%B7.md) (Python ä»£ç æ€§èƒ½åˆ†æ)

## ğŸ·ï¸ Linux ä½¿ç”¨
* [Linux å°±è¯¥è¿™ä¹ˆå­¦](https://www.linuxprobe.com/) (å…è´¹PDFæ•™æ)
* [Oh-my-zsh](https://zhuanlan.zhihu.com/p/35283688) ğŸš€ (é…ç½®å¥½ç”¨çš„å‘½ä»¤è¡Œ)
* [Tmux](https://zhuanlan.zhihu.com/p/98384704) (è¿œç¨‹è¿æ¥æœåŠ¡å™¨åå°è¿è¡Œä»£ç ) [ä½¿ç”¨æ‰‹å†Œ](http://louiszhai.github.io/2017/09/30/tmux/)

## ğŸ·ï¸ Github :octocat:
* [Best-README-Template](https://github.com/yzfly/Best-README-Template)

## ğŸ·ï¸ Others
* [Cuda å®‰è£…å’Œé—®é¢˜è§£å†³](./nvidia_gpu.md)

* [Anaconda å®‰è£…ä½¿ç”¨](https://blog.csdn.net/a745233700/article/details/109376667)(æ–¹ä¾¿ Python ç¯å¢ƒç®¡ç†)

* [VS Code è¿œç¨‹å¼€å‘](https://zhuanlan.zhihu.com/p/141344165) (è¿œç¨‹è¿æ¥æœåŠ¡å™¨å¼€å‘ç¨‹åºï¼Œ PyCharm ä¹Ÿå…·å¤‡è¯¥åŠŸèƒ½)